---
tags: Diary, Periodicals
tracked: 1
---

## Todos
- [x] Pickup the keys and stuff to Amed
- [x] Resolving the conflicts generated by incorrectly using interactive rebase
- [x] Rendezvous with Michi and kids in Amed in the first hotel
- [x] Settle in the new location, Bluemoon Vlilla, Amed.

<iframe src="https://www.google.com/maps/embed?pb=!1m18!1m12!1m3!1d1385.9928251229999!2d115.6927073536737!3d-8.357586313528163!2m3!1f0!2f0!3f0!3m2!1i1024!2i768!4f13.1!3m3!1m2!1s0x2dcdff3c4f9f40a7%3A0x7cc5287ca98980f6!2sBlue%20Moon%20Villas!5e0!3m2!1sen!2s!4v1689836223247!5m2!1sen!2s" width="600" height="450" style="border:0;" allowfullscreen="" loading="lazy" referrerpolicy="no-referrer-when-downgrade"></iframe>



Luke Fan told me to not spend time on development, but on specification writing. This is the right suggestion. Moreover, I must focus on writing the specification of having a quality control workflow, or Automated QA. Emphasizing on the correctness verification of all projects.

Projects are stateful systems, therefore, creating tools to monitor the states of these systems would be the focus on these specifications.

## The 1% Inspiration that leads to revelations
Bt 5:02 PM, I realized that all processes can be annotated based on the [[REPL]] cycle. This is a huge revelation because it allows me to see that all activities can be organized in these annotated data structure, that can be both represented as functions, and tested and recorded as function executing results. This also enables anyone and any projects to be abstractly represented as a unifying data structure, so that testing and annotating can be done in a [[Universality|universal]] manner.
#### Implementation ideas
This also leads to the idea that one can build a shadow operating system using Docker containers, so that ANY directory on a filesystem can be mapped automatically to a chosen PKC container. This mapping automates the synchronization of data content, so that we can leverage all the features of Obsidian-like data navigating feature to inspect the content, and annotate the content with markdown syntax. Moreover, certain set of functions can be annotated using [[Zencode]], to add cryptographic signatures to specify the timing and sequencing order to processing actions, so that execution sequences can be faithfully captured and represented in certain order-preserving log data guaranteed by cryptographic data structures. This data structure can be further linked with Git-like version control system to guarantee timing sequences of data generation, so that we can use the account information of Git change history as the evidence for social accountability, and the sequential ordering information as the physical meaning of data.
#### File as the Universal Data Abstraction
Thinking of any data content in terms of file and file only is a critical leap in this design. Obsidian treats directories, a file that contains other files, to be the basic unit of data vault. We nee to leverage this idea to the next level. All data content are files. This way, we can not only leverage the [[Content Addressing]] notion of [[IPFS]], we may also start having a generalized way to compare and contrast data content of all kinds. This will accelerate the integration and accounting process of data assets. It will also help in organizing content compilation in the [[Langchain]]-based data refinement process. In this case, [[Langchain]] should be implemented in a design pattern based on [[REPL]]. In other words, from the scale of any primitive operations on data, such as adding two integer numbers, to complex data transformation, such as calling [[GPT4All]]-kind of APIs, they should all be wrapped around some [[REPL]] pattern, and governed by the same quality control process witnessed or annotated by [[Zencode]]. This will become the ultimate kernel of decisions, and therefore minimize the effort to compare and contrast different kinds of computational anomalies caused by variations in execution environments or conflicting interpretations.
#### Zencode-driven workflow
The simplicity of [[Zencode]] allows us to annotate processes of any scale with a causal relation, and embed a cryptographic code to witness the social accountability and physical occurrence sequence. It will realize the original vision of [[Literate programming|Literate programming]]. This is particularly useful for human to utilize these literary notes because it will relate activities of all executable code to natural language statements, so that over time, with sufficient execution log accumulation, we can reason about the meaning of all temporal ordering of events, as well as socially attached accounts to some generic renormalized procedure, so that we can leverage GPTs to reason about activities in the most unbiased manner. This will become the [[Logic Kernel]] that understands the social and physical meaning of data. Moreover, we can integrate tools such as [[PKC]] and [[Obsidian]] to contain the data assets, as well as navigating these data content in a human friendly manner at scale.

### Operationalize Correctness
The whole point of creating the [[Logic Kernel]] is to operationalize the means to examine [[Correctness]] using available data and computational tools. Specifically, we will integrate this whole process with the knowledge and instruments of [[DevOps]]. The [[Logic Kernel]] along with [[PKC]] will help standardize the activities of managing data assets with [[Correctness]] across all levels of activities.

### Integrate DevOps with Smart Contract
The main differences between DevOps and DevSecOps are related to how security features are managed. One way to accomplish security at scale, is to integrate [[DID]] with [[DevOps]], so that we can have a privacy protecting identity management system that can be mapped onto a wide range of [[DevOps]] activities, in other words: [[Privacy by Design]]. This design principle can also be integrated with [[Zenroom]] to supply the cryptographic verification and authentication functions. Ideally, the reward system could be integrated with some [[Blockchain]]-based payment system, or certain [[Smart Contract]] system such as [[Ethereum]] or [[Polygon]]-based smart contract infrastructure, so that all participants can be held accountable based on some [[DID]]-based identity management system. A [[DevOps]] process that has built-in payment transaction features, along with some [[Non-Fungible Token|NFT]] cataloging system, will basically transform [[DevSecOps]] into a true [[Web3]] platform.

#### UI/UX for Data Manipulation (including Diagramming)
As I watch [The Modern Coder Youtube Channel](https://www.youtube.com/@TheModernCoder/about), it reminds me of using diagrams to illustrate the general case of data manipulation, which is basically the manipulate of [[Lattice|lattices]]. One can learn to use Git and get familiar with all the commands and verbs related to data manipulation, for example, also from Jack's website:[LearnGit.IO](https://waitlist.learngit.io/) (who is [The Modern Coder](https://www.youtube.com/@TheModernCoder/about)).  and then [[Obsidian]]'s canvas feature to create user interfaces interactively. Ideally, I could create an [[Obsidian]] plug-in and a [[PKC]] service, so that one may draw the diagram in Obsidian Canvas, but basically have the same functionalities of [[Blender]]'s [[Geometry Nodes]]. By using the Markdown syntax as the data content carrier, it should reduce a significant amount of developer efforts. The main idea is that the entire workflow of creating and refinement of new products and services, should be a [[PKC]] supported file manipulation and file version tagging process. This process should be supported by something very similar to [[Obsidian]], a file manipulation, annotation, and content browsing/navigation system. This will elevate [[Obsidian]] from a merely note taking software, to a new kind of shell or file finder for data-centric/content centric operating systems. This idea should support many profound statements made by [[John Day]], the guy who wrote: [[@dayPatternsNetworkArchitecture2008|Patterns in network architecture]].

#### Refinement with Langchain
All the above content should be shuffled into [[ChatGPT]], and other similarly capable [[Large Language Model|LLM]]s in the following sequence:
1. Ask ChatGPT to read this note and reorganize it into an outline of an engineering project requirement document. Including using the [[Mix of Experts]] way to improve the results using [[Langchain]] for integration.
2. Take each section, of the generated and original textual content, use multiple [[Large Language Model|LLM]]s to refine the content, and create an initial, and yet, minimally viable engineering project proposal.
3. Do all these work of changing the content, as much as possible through the version control system of [[Obsidian]] vaults, and use Git as the primary version control tool. Use and create plug-ins to improve the integration of these two ways of file management.
4. Try to get users to think of all knowledge assets in terms of [[File|files]]. Then, the overall workflow is just to keep refining the content of various collections of files in various [[Obsidian]] vaults (directories).
5. Mobilize more people to participate in the data asset refinement exercises, by adopting the above mentioned [[DID]]-based [[DevOps]] practice, and use [[Smart Contract]]s to regulate and motivate participates to contribute and critic the content of various data content packages in an integrate workflow.
6. In the process of doing so, improve the workflow by incrementally changing the note templates in [[Obsidian]], and use note templates as the means to enable [[Prompt Engineering]].
7. Wrap the whole thing into an [[Algebra of Systems]]([[Algebra of Systems|AoS]]), so that we can have closure as the initial and required condition through out the process and space of design search activities. This is also considered to be a [[Correct by Design]] or [[Privacy by Design]] idea. 


## Before Going to Bed
Found [[@GitHubBigscienceworkshopPetals|Petals.ML]] a project for running large language models distributedly.




### File Generated Time
2023-07-20 08:16

#Diary 

## References

[[@dayPatternsNetworkArchitecture2008]]
