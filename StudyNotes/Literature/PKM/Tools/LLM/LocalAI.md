[LocalAI](https://localai.io/) is a drop-in replacement REST API thatâ€™s compatible with OpenAI API specifications for local inferencing. It allows you to run LLMs (and not only) locally or on-prem with consumer grade hardware, supporting multiple model families that are compatible with the ggml format. Does not require GPU.

## Working with Copilot in Obsidian
[[LocalAI]] can serve as the intelligent agent to provide interactive feedback through [[Copilot for Obsidian]].

## Author
[[Ettore Di Giacinto]] is the creator of [[LocalAI]].