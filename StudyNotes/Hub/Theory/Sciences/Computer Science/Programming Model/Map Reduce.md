---
Aliases: map reduce
---
#reducer

[MapReduce](https://www.wikiwand.com/en/MapReduce) is a programming model and an associated implementation for processing and generating large data sets in a distributed computing environment. It was popularized by Google and has become widely used in the field of big data.

In MapReduce, the processing of data is divided into two main stages: the Map stage and the Reduce stage.

1. Map Stage: In this stage, the input data is divided into smaller chunks, which are then processed independently by multiple map tasks. Each map task applies a user-defined function called the "map function" to each input chunk, producing intermediate key-value pairs as output. The map function can perform filtering, sorting, or any other necessary transformations on the data.

2. Reduce Stage: In this stage, all intermediate key-value pairs generated by the map tasks are grouped together based on their keys. Multiple reduce tasks then process these groups independently. Each reduce task applies a user-defined function called the "reduce function" to each group of intermediate values with the same key. The reduce function typically performs aggregation or summarization operations on the data.

The results of the reduce tasks are combined to produce the final output of the MapReduce job. This output can be stored in a file system or used as input for subsequent MapReduce jobs.

MapReduce provides automatic parallelization and distribution of data and computation across a cluster of machines. It handles fault tolerance by automatically re-executing failed tasks on other machines in the cluster.

While originally designed for batch processing of large datasets, MapReduce has also been extended to support real-time processing with frameworks like [Apache Hadoop](https://www.wikiwand.com/en/Apache_Hadoop), Apache Storm and Apache Flink.

Overall, MapReduce simplifies distributed data processing by abstracting away many complexities of parallelization and fault tolerance, allowing developers to focus on writing simple map and reduce functions to process their data efficiently.

## How does map reduce inform or influence the design of Reducers in React.js and Redux?

MapReduce is a programming model and algorithm for processing and generating large data sets in parallel. It inspired the design of reducers in React.js and Redux to some extent, but there are also differences in their implementation.

In MapReduce, the processing is divided into two main steps: map and reduce. The map step performs filtering and sorting of the input data, while the reduce step performs computation on the filtered and sorted data. This approach allows for distributed processing and scalability.

In React.js and Redux, reducers are functions that specify how the application's state changes in response to actions. They take the current state and an action as input, and return a new state based on that action. This is somewhat similar to the reduce step in MapReduce, where computation is performed on filtered data.

However, there are some differences between MapReduce and reducers in React.js/Redux. In MapReduce, the map step filters data based on specific criteria before passing it to reducers. In React.js/Redux, actions can be dispatched from different parts of the application without any explicit filtering. Reducers handle all actions and decide how to update the state accordingly.

Additionally, MapReduce is typically used for big data processing where parallelism is crucial. React.js/Redux are primarily used for managing state within a single application or component hierarchy. While reducers can handle complex logic using techniques like memoization or reselect libraries, they do not inherently provide distributed processing capabilities like MapReduce.

In summary, while MapReduce influenced the concept of reducers in React.js/Redux by emphasizing computation on filtered data, there are differences in their implementation due to the distinct goals and contexts they are designed for.