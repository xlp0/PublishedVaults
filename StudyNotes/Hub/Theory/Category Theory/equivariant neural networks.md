#Symmetry 

Equivariant neural networks are a type of deep learning model that possess a specific symmetry property called equivariance. In traditional neural networks, the input data is transformed into feature representations through multiple layers of non-linear transformations. However, these transformations do not consider the underlying symmetries present in the data.

Equivariant neural networks, on the other hand, are designed to respect and preserve these symmetries. They have the ability to learn and exploit the symmetries in the input data during training, resulting in more efficient and effective models for certain tasks.

The concept of equivariance refers to how the output of a neural network changes when the input undergoes a transformation. In an equivariant neural network, if the input is transformed in some way (e.g., rotated or translated), then the output is transformed in a corresponding manner. This property allows equivariant networks to generalize better to new examples by leveraging symmetry-based knowledge.

Equivariant neural networks have been successfully applied in various domains where symmetry plays a crucial role, such as computer vision, physics simulations, and molecular chemistry. By explicitly considering and exploiting symmetry properties, these models can achieve superior performance compared to traditional neural networks that do not incorporate such considerations.

# References

[[@michaelbronsteinLectureEquivariantCNNs2022|Lecture on Equivariant CNN]]